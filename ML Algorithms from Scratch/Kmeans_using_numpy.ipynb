{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means steps:\n",
    "# 1. initialize:\n",
    "# - k = number of clusers\n",
    "# - Data = Data to cluster\n",
    "# - iterations - no. of iterations\n",
    "\n",
    "# 2. repeat:\n",
    "# - initiate k random centroids from data\n",
    "# - calculate euclidiean distance b/w each data point and centroids and assign that data point to centroid/cluster with minimum distance\n",
    "# - calculate new centroid - mean of all datapoints\n",
    "# - untill old centroid == new centroid or no. of iterations\n",
    "\n",
    "# 3. Output:\n",
    "# - final k clusters and their corresponding centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets as ds\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=gnGyTRFLX-k\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, k=3, iterations=2000):\n",
    "        self.k = k\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def euclideanDistance(self, v1, v2):\n",
    "        return np.sqrt(np.sum((v1-v2)**2))\n",
    "\n",
    "    def createClusters(self, centroids):\n",
    "        clusters = [[] for _ in range(self.k)]\n",
    "        for i, samples in enumerate(self.X):\n",
    "            distances = [self.euclideanDistance(samples, c) for c in centroids]\n",
    "            cluster_idx = np.argmin(distances)\n",
    "            clusters[cluster_idx].append(i)\n",
    "\n",
    "        return clusters\n",
    "\n",
    "    def getNewCentroid(self, clusters):\n",
    "        new_centroids = np.zeros((self.k, self.n_features), dtype=np.float64)\n",
    "        for i, c in enumerate(clusters):\n",
    "            cluster_mean = np.mean(self.X[c], axis=0)\n",
    "            new_centroids[i] = cluster_mean\n",
    "\n",
    "        return new_centroids\n",
    "\n",
    "    def isConverged(self, old_centroids, new_centroids):\n",
    "        distance = [self.euclideanDistance(old_centroids[i], new_centroids[i]) for i in range(self.k)]\n",
    "        return np.sum(distance) == 0\n",
    "\n",
    "    def getLabels(self, clusters):\n",
    "        n_labels = np.empty(self.n_samples)\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            for sample in cluster:\n",
    "                n_labels[sample] = i\n",
    "\n",
    "        return n_labels\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.X = X\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        rand_idx = np.random.choice(self.n_samples, self.k, replace=False)\n",
    "        centroids = [self.X[i] for i in rand_idx]\n",
    "\n",
    "        for _ in range(self.iterations):\n",
    "            clusters = self.createClusters(centroids)\n",
    "            old_centroids = centroids\n",
    "            centroids = self.getNewCentroid(clusters)\n",
    "\n",
    "            if self.isConverged(old_centroids, centroids):\n",
    "                break\n",
    "\n",
    "        return self.getLabels(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = ds.make_blobs(n_samples=150, n_features=2, centers=3, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true==y_pred)/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans()\n",
    "y_pred = kmeans.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.35)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "An interviewer assessing your K-Means algorithm implementation for a senior ML engineer role might probe for deeper optimizations or extensions. Here are suggested follow-up optimizations and improvements:\n",
    "\n",
    "1. Initialization Improvements\n",
    "K-Means++ initialization:\n",
    "Instead of random initialization, use K-Means++ to choose better initial centroids. This can significantly reduce the chances of getting stuck in local minima and improve convergence.\n",
    "\n",
    "2. Optimized Distance Calculation\n",
    "Squared distances:\n",
    "Optimize by removing the square root operation in your distance calculations (use squared distances to speed up comparisons).\n",
    "\n",
    "3. Early stopping with Tolerance\n",
    "Use a convergence tolerance (tol) instead of strict zero-difference checks, allowing faster convergence.\n",
    "\n",
    "4. Handling Empty Clusters\n",
    "Check for and handle cases when a cluster becomes empty by reinitializing that centroid.\n",
    "\n",
    "5. Vectorization\n",
    "Fully vectorize calculations with NumPy to reduce Python loops and significantly boost performance.\n",
    "\n",
    "6. Parallelization\n",
    "Explore parallel implementations (e.g., via joblib or multiprocessing) for computing distances, especially for large datasets.\n",
    "\n",
    "7. Elbow Method and Silhouette Analysis\n",
    "Incorporate evaluation methods like the elbow method or silhouette score for automatically determining the optimal number of clusters (k).\n",
    "\n",
    "8. Scalability with Mini-Batch K-Means\n",
    "Discuss the potential use of Mini-Batch K-Means, especially relevant for large-scale datasets.\n",
    "\n",
    "9. Caching Intermediate Results\n",
    "Store computations of frequently accessed results, such as cluster assignments or distances, to avoid redundant computations across iterations.\n",
    "\n",
    "10. Robustness Checks\n",
    "Introduce methods to validate inputs, ensuring robustness against anomalies (e.g., NaN values, infinite values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
